{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Mouse Pointer Detction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Frame Differential Screening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video file path\n",
    "video_path = 'testvideo2.mkv'\n",
    "\n",
    "# Create a folder to save the moving object frames\n",
    "moving_frame_folder = 'moving_frames'\n",
    "if not os.path.exists(moving_frame_folder):\n",
    "    os.makedirs(moving_frame_folder)\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "ret, frame1 = cap.read()\n",
    "gray_frame1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "frame_count = 0\n",
    "frame_count += 1\n",
    "\n",
    "# Create a dictionary to store the locations of moving objects in each frame\n",
    "locations = {}\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame2 = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    gray_frame2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Calculate the difference between two consecutive frames\n",
    "    frame_diff = cv2.absdiff(gray_frame1, gray_frame2)\n",
    "    \n",
    "    # Apply a threshold to the difference image, in order to mark the changed area\n",
    "    _, thresh = cv2.threshold(frame_diff, 25, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "    thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    # Find contours in the threshold image\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Draw bounding boxes around the contours\n",
    "    for contour in contours:\n",
    "        if cv2.contourArea(contour) < 110 or cv2.contourArea(contour) > 400:\n",
    "            continue\n",
    "        (x, y, w, h) = cv2.boundingRect(contour)\n",
    "        cv2.rectangle(frame2, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
    "        \n",
    "        if frame_count not in locations:\n",
    "            locations[frame_count] = []\n",
    "        locations[frame_count].append((x, y))\n",
    "        \n",
    "        position_text = f\"({x}, {y})\"\n",
    "        cv2.putText(frame2, position_text, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "    \n",
    "        # Save the frame\n",
    "        output_frame_path = os.path.join(moving_frame_folder, f\"frame_{frame_count}.jpg\")\n",
    "        cv2.imwrite(output_frame_path, frame2)\n",
    "        \n",
    "    frame_count += 1\n",
    "    gray_frame1 = gray_frame2\n",
    "\n",
    "print(f\"Processed {frame_count} frames\")\n",
    "print(locations)\n",
    "\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Intra-frame Clustering by Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centers = {}\n",
    "\n",
    "# Calculate the cluster center of moving objects in each frame\n",
    "for frame, points in locations.items():\n",
    "    points_array = np.array(points)\n",
    "    \n",
    "    # If there is only one moving object in the frame, the cluster center is exactly the location of the moving object\n",
    "    if len(points) == 1:\n",
    "        cluster_centers[frame] = points[0]\n",
    "    else:\n",
    "        # If there are multiple moving objects in the frame, use KMeans to calculate the cluster center\n",
    "        kmeans = KMeans(n_clusters=1, random_state=0).fit(points_array)\n",
    "        cluster_centers[frame] = tuple(map(int, np.round(kmeans.cluster_centers_[0]))) \n",
    "\n",
    "print(cluster_centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Inter-frame Clustering by DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN parameters\n",
    "eps = 20  # neighborhood distance\n",
    "min_samples = 2  # minimum number of samples required to form a dense region\n",
    "\n",
    "new_cluster_centers = {}\n",
    "\n",
    "keys = list(cluster_centers.keys())\n",
    "\n",
    "# Iterate through each frame\n",
    "for i, frame in enumerate(keys):\n",
    "    \n",
    "    # Get the window frames for the current frame (3 frames before and 3 frames after)\n",
    "    window_frames = keys[max(i - 3, 0):min(i + 4, len(keys))]\n",
    "    print(f\"Window frames for frame {frame}: {window_frames}\")\n",
    "    \n",
    "    # Get the cluster center points for the window frames\n",
    "    window_points = [cluster_centers[f] for f in window_frames]\n",
    "    print(f\"Window points for frame {frame}: {window_points}\")\n",
    "    \n",
    "    # Convert the points to a numpy array\n",
    "    points_array = np.array(window_points)\n",
    "    \n",
    "    # Use DBSCAN to cluster the points\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples).fit(points_array)\n",
    "    labels = dbscan.labels_\n",
    "    \n",
    "    # If all labels are -1 (noise points), use the original cluster center\n",
    "    if np.all(labels == -1):\n",
    "        new_cluster_centers[frame] = cluster_centers[frame]\n",
    "    else:\n",
    "        # Count the number of points in each cluster, determine the largest cluster\n",
    "        label_count = Counter(labels[labels != -1])\n",
    "        if label_count:\n",
    "            most_common_label = label_count.most_common(1)[0][0]\n",
    "            most_common_points = points_array[labels == most_common_label]\n",
    "            \n",
    "            # Calculate the new cluster center using the mean of the points in the largest cluster\n",
    "            new_center = np.mean(most_common_points, axis=0)\n",
    "            new_cluster_centers[frame] = tuple(map(int, np.round(new_center))) \n",
    "        else:\n",
    "            # If there is no cluster, use the original cluster center\n",
    "            new_cluster_centers[frame] = cluster_centers[frame]\n",
    "\n",
    "for frame, center in new_cluster_centers.items():\n",
    "    print(f\"Frame {frame}: New Cluster Center {center}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the bounding box around the moving object in the frame, save the original image at the same time\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "frame_count = 0\n",
    "moving_cluster_frame_folder = 'moving_cluster_frames'\n",
    "if not os.path.exists(moving_cluster_frame_folder):\n",
    "    os.makedirs(moving_cluster_frame_folder)\n",
    "moving_cluster_original_frames_folder = 'moving_cluster_original_frames'\n",
    "if not os.path.exists(moving_cluster_original_frames_folder):\n",
    "    os.makedirs(moving_cluster_original_frames_folder)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    frame_count += 1\n",
    "    \n",
    "    if frame_count in new_cluster_centers:\n",
    "        output_original_frame_path = os.path.join(moving_cluster_original_frames_folder, f\"frame_{frame_count}.jpg\")\n",
    "        cv2.imwrite(output_original_frame_path, frame)\n",
    "        \n",
    "        center = new_cluster_centers[frame_count]\n",
    "        cv2.rectangle(frame, (center[0]-10, center[1]-10), (center[0]+10, center[1]+10), (0, 255, 0), 2)\n",
    "        \n",
    "        output_frame_path = os.path.join(moving_cluster_frame_folder, f\"frame_{frame_count}.jpg\")\n",
    "        cv2.imwrite(output_frame_path, frame)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2: Global Frame Differential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIFF_THRESHOLD = 0.2\n",
    "\n",
    "key_frame_folder = 'key_frames'\n",
    "if not os.path.exists(key_frame_folder):\n",
    "    os.makedirs(key_frame_folder)\n",
    "\n",
    "def calculate_frame_difference(frame1, frame2):\n",
    "    \n",
    "    diff = cv2.absdiff(frame1, frame2)\n",
    "    \n",
    "    diff_value = np.mean(diff)\n",
    "    \n",
    "    return diff_value\n",
    "\n",
    "def detect_key_frames(video_path, threshold=DIFF_THRESHOLD):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    frame_count = 0\n",
    "    detect_count = 0\n",
    "    key_frames = []\n",
    "    \n",
    "    ret, prev_frame = cap.read()\n",
    "    frame_count += 1\n",
    "    \n",
    "    while ret:\n",
    "        ret, current_frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Calculate the difference between two consecutive frames\n",
    "        diff_value = calculate_frame_difference(prev_frame, current_frame)\n",
    "        # print(f\"Key frame detected at frame {frame_count}, diff: {diff_value}\")\n",
    "        \n",
    "        # Compare the difference with the threshold\n",
    "        if diff_value > threshold:\n",
    "            # Make sure the time interval between keyframes is at least 10 frames\n",
    "            if frame_count - detect_count > 11:\n",
    "                key_frames.append(frame_count)\n",
    "                frame_filename = os.path.join(key_frame_folder, f\"frame_{frame_count}.jpg\")\n",
    "                cv2.imwrite(frame_filename, current_frame)\n",
    "                print(f\"Key frame detected at frame {frame_count}, diff: {diff_value}\")\n",
    "                \n",
    "            detect_count = frame_count\n",
    "        \n",
    "        prev_frame = current_frame\n",
    "        frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    return key_frames\n",
    "\n",
    "video_path = 'testvideo2.mkv'\n",
    "key_frames = detect_key_frames(video_path)\n",
    "\n",
    "print(f\"Detected {len(key_frames)} key frames: {key_frames}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3: Local Frame Differential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_pairs = []\n",
    "\n",
    "keys = list(new_cluster_centers.keys())\n",
    "\n",
    "for i, frame in enumerate(keys):\n",
    "    window_frames = keys[max(i - 1, 0):i+1]\n",
    "    print(f\"Window frames for frame {frame}: {window_frames}\")\n",
    "    \n",
    "    window_points = [new_cluster_centers[f] for f in window_frames]\n",
    "    print(f\"Window points for frame {frame}: {window_points}\")\n",
    "    \n",
    "    for j in range(len(window_points) - 1):\n",
    "        for k in range(j + 1, len(window_points)):\n",
    "            point1 = window_points[j]\n",
    "            point2 = window_points[k]\n",
    "            distance = np.linalg.norm(np.array(point1) - np.array(point2))\n",
    "            print(distance)\n",
    "            if distance < 3:\n",
    "                moving_pairs.append((window_frames[j], window_frames[k]))\n",
    "\n",
    "print(moving_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_frame_original_folder = 'moving_cluster_original_frames'\n",
    "color_threshold = 30\n",
    "color_pixel_threshold = 20\n",
    "local_key_frames = []\n",
    "\n",
    "patch_folder = 'patch_frames'\n",
    "if not os.path.exists(patch_folder):\n",
    "    os.makedirs(patch_folder)\n",
    "\n",
    "# Determine if a pixel is a color pixel\n",
    "def is_color_pixel(pixel, threshold=15):\n",
    "    b, g, r = pixel\n",
    "    return (abs(int(r) - int(g)) > threshold or \n",
    "            abs(int(r) - int(b)) > threshold or \n",
    "            abs(int(g) - int(b)) > threshold)\n",
    "\n",
    "for pair in moving_pairs:\n",
    "    frame1_path = os.path.join(moving_frame_original_folder, f\"frame_{pair[0]}.jpg\")\n",
    "    frame1 = cv2.imread(frame1_path)\n",
    "    \n",
    "    frame2_path = os.path.join(moving_frame_original_folder, f\"frame_{pair[1]}.jpg\")\n",
    "    frame2 = cv2.imread(frame2_path)\n",
    "    \n",
    "    # Extract the patch (40x40) around the center\n",
    "    center = new_cluster_centers[pair[1]]\n",
    "    x, y = center\n",
    "    x1, y1 = x - 20 , y - 20\n",
    "    x2, y2 = x + 20, y + 20\n",
    "    \n",
    "    patch1 = frame1[y1:y2, x1:x2]\n",
    "    patch2 = frame2[y1:y2, x1:x2]\n",
    "    \n",
    "    patch1_path = os.path.join(patch_folder, f\"patch1_{pair[0]}.jpg\")\n",
    "    cv2.imwrite(patch1_path, patch1)\n",
    "    patch2_path = os.path.join(patch_folder, f\"patch2_{pair[1]}.jpg\")\n",
    "    cv2.imwrite(patch2_path, patch2)\n",
    "    \n",
    "    # Calculate the average of the color pixels in the patch\n",
    "    color_pixel1 = [pixel for pixel in patch1.reshape(-1, 3) if is_color_pixel(pixel, color_pixel_threshold)]\n",
    "    color_pixel2 = [pixel for pixel in patch2.reshape(-1, 3) if is_color_pixel(pixel, color_pixel_threshold)]\n",
    "    avg_color1 = np.mean(color_pixel1, axis=0)\n",
    "    avg_color2 = np.mean(color_pixel2, axis=0)\n",
    "    color_diff = np.linalg.norm(avg_color1 - avg_color2)\n",
    "    \n",
    "    # Compare the color difference with the threshold\n",
    "    if color_diff > color_threshold:\n",
    "        print(f\"Key frame detected at frame {pair[1]}, diff:{color_diff}\")\n",
    "        local_key_frames.append(pair[1])\n",
    "\n",
    "print(local_key_frames)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4: Keyframe Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_keyframes = key_frames.copy()\n",
    "# Aggregate the key frames detected by local color difference and global difference\n",
    "# If there is no global keyframe within 5 frames of the local keyframe, add it to the final list\n",
    "for local_key in local_key_frames:\n",
    "    if all(abs(local_key - key) >= 5 for key in key_frames):\n",
    "        merged_keyframes.append(local_key)\n",
    "\n",
    "merged_keyframes = sorted(merged_keyframes)\n",
    "print(merged_keyframes)\n",
    "# Create a dictionary to store the final cluster center of each key frame\n",
    "final_pointer = {}\n",
    "for key_frame in merged_keyframes:\n",
    "    if key_frame in new_cluster_centers:\n",
    "        final_pointer[key_frame] = new_cluster_centers[key_frame]\n",
    "        print(f\"Key frame {key_frame}: {new_cluster_centers[key_frame]}\")\n",
    "    # If the key frame is not in the new cluster center, find the closest frame\n",
    "    else:\n",
    "        closest_frame = min(new_cluster_centers.keys(), key=lambda x: abs(x - key_frame))\n",
    "        final_pointer[key_frame] = new_cluster_centers[closest_frame]\n",
    "        print(f\"Key frame {key_frame}: {new_cluster_centers[closest_frame]}\")\n",
    "\n",
    "final_frame_folder = 'final_frames'\n",
    "if not os.path.exists(final_frame_folder):\n",
    "    os.makedirs(final_frame_folder)\n",
    "    \n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "frame_count = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    if frame_count in final_pointer.keys():\n",
    "        x, y = final_pointer[frame_count]\n",
    "        x1, y1 = x, y\n",
    "        # 指针矩阵大小为12x20\n",
    "        x2, y2 = x + 12, y + 20\n",
    "\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "\n",
    "        output_frame_path = os.path.join(final_frame_folder, f\"frame_{frame_count}.jpg\")\n",
    "        cv2.imwrite(output_frame_path, frame)\n",
    "\n",
    "    frame_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 5: OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import easyocr\n",
    "import os\n",
    "import PIL\n",
    "PIL.Image.ANTIALIAS = PIL.Image.LANCZOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = 'final_frames'\n",
    "output_folder = 'text_square_frames'\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Initialize the EasyOCR reader\n",
    "reader = easyocr.Reader(['en'])\n",
    "\n",
    "# Create a dictionary to store the text and location information of each frame\n",
    "text_location = {}\n",
    "\n",
    "for filename in merged_keyframes:\n",
    "    image_path = os.path.join(input_folder, f\"frame_{filename}.jpg\")\n",
    "    image = cv2.imread(image_path)\n",
    "    results = reader.readtext(image)\n",
    "\n",
    "    for (bbox, text, prob) in results:\n",
    "        (top_left, top_right, bottom_right, bottom_left) = bbox\n",
    "        top_left = tuple(map(int, top_left))\n",
    "        bottom_right = tuple(map(int, bottom_right))\n",
    "\n",
    "        cv2.rectangle(image, top_left, bottom_right, (0, 255, 0), 2)\n",
    "        \n",
    "        if filename not in text_location:\n",
    "            text_location[filename] = []\n",
    "\n",
    "        text_location[filename].append((text, top_left))\n",
    "        \n",
    "        output_path = os.path.join(output_folder, f\"frame_{filename}.jpg\")\n",
    "        cv2.imwrite(output_path, image)\n",
    "\n",
    "print(text_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_pointer)\n",
    "print(text_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 6: Prompt Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prompt1 = \"This is a video about a laboratory workflows on a software interface, and I need you to understand each step in the workflow. I have done some preliminary analysis and will provide some information to you later, including key frames you need to focus, the coordinates of mouse pointer, text and the coordinates of text .\" \n",
    "\n",
    "Prompt2 = \"Firstly, I have captured the keyframes and the corresponding coordinates of mouse pointer in this video. Data form is {frame_number: (x, y)}. Your first task is to use this information to generate 'pyautogui' code to simulate the click process.\"\n",
    "Prompt2 = f\"{Prompt2} {final_pointer}\"\n",
    "print(Prompt2)\n",
    "\n",
    "Prompt3 = \"Secondly, I have extracted the text in the images and their coordinates. Data form is {frame_number: [text,(x, y)]}. Your second task is to match the text information to the corresponding step in the workflow.\"\n",
    "Prompt3 = f\"{Prompt3} {text_location}\"\n",
    "print(Prompt3)\n",
    "\n",
    "Prompt4 = \"Thirdly, there are several steps missing before clicking 'Apply' in the workflow, they are: 1. add discrete amounts of B in ul to Plate 1; 2. add discrete amounts of C in ul to Plate 1; 3. add discrete amounts of D in ul to Plate 1. Your third task is to only find the key frame that need to be revised and add the missing steps before clicking 'Apply'.\"\n",
    "\n",
    "Prompt5 = \"Finally, Your final task is to generate the revised 'pyautogui' code of the whole process, noted that if you want to check these missing steps, click the check box about 390 pixels to the right of the matching text.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 7: Generate Object Code by using LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from IPython.display import display, Image, Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url=\"https://oneapi.xty.app/v1\",\n",
    "    api_key=\"your_api_key \" # your api key \n",
    ")\n",
    "\n",
    "PROMPT_MESSAGES = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\n",
    "            \"type\": \"text\",\n",
    "            \"text\": Prompt1,\n",
    "            # *map(lambda x: {\"image\": x, \"resize\": 768}, base64Frames),\n",
    "        }],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\n",
    "            \"type\": \"text\",\n",
    "            \"text\": Prompt2,\n",
    "        }],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\n",
    "            \"type\": \"text\",\n",
    "            \"text\": Prompt3,\n",
    "        }],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\n",
    "            \"type\": \"text\",\n",
    "            \"text\": Prompt4,\n",
    "        }],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\n",
    "            \"type\": \"text\",\n",
    "            \"text\": Prompt5,\n",
    "        }],\n",
    "    },\n",
    "]\n",
    "temp = 0.0\n",
    "penalty =0.0\n",
    "messages = []\n",
    "for message in PROMPT_MESSAGES:\n",
    "    messages.append(message)\n",
    "    params = {\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": 2048,\n",
    "        \"temperature\": temp,\n",
    "        \"presence_penalty\": 1.0,\n",
    "    }\n",
    "    print(result.choices[0].message.content)\n",
    "    print(\"**************Next Step**************\")\n",
    "    result = client.chat.completions.create(**params)\n",
    "    messages.append({\"role\":\"assistant\",\"content\":result.choices[0].message.content})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IERes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
